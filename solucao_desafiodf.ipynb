{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c589e8-f70d-4a70-9ea7-7bd7eb6bc6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Setup: instalar libs\n",
    "# =========================\n",
    "# Observação: em Kaggle, muitas vezes já existe parte disso instalado.\n",
    "# O -q deixa a saída mais limpa. Remova se quiser ver logs.\n",
    "!pip -q install -U transformers datasets seqeval scikit-learn accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3899cb7b-2784-4309-b584-ea73de164336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.10.0+cpu\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2) Imports e configurações\n",
    "# =========================\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score as seqeval_accuracy\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score as sk_accuracy,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE_ID = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba943134-5401-4342-8042-8ad57b333815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Utilitários: localizar arquivos e preparar dados NER\n",
    "# =========================\n",
    "def find_file_exact_or_pattern(\n",
    "    filename: str,\n",
    "    patterns: List[str],\n",
    "    base_dirs: List[Path],\n",
    ") -> Path:\n",
    "    \"\"\"Procura um arquivo por nome exato e por padrões (glob), recursivamente.\"\"\"\n",
    "    # 1) Checagem direta\n",
    "    for d in base_dirs:\n",
    "        p = d / filename\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    # 2) Busca recursiva por nome exato\n",
    "    for d in base_dirs:\n",
    "        if d.exists():\n",
    "            hits = list(d.rglob(filename))\n",
    "            if hits:\n",
    "                return hits[0]\n",
    "\n",
    "    # 3) Busca por padrões (glob)\n",
    "    for pat in patterns:\n",
    "        for d in base_dirs:\n",
    "            if d.exists():\n",
    "                hits = list(d.rglob(pat))\n",
    "                if hits:\n",
    "                    return hits[0]\n",
    "\n",
    "    existing = [str(d) for d in base_dirs if d.exists()]\n",
    "    raise FileNotFoundError(\n",
    "        f\"Não encontrei '{filename}' (nem padrões {patterns}) nos diretórios: {existing}.\\n\"\n",
    "        f\"Dica: no Kaggle, os arquivos costumam estar em /kaggle/input/<dataset>/...\"\n",
    "    )\n",
    "\n",
    "def tokenize_with_spans(text: str) -> Tuple[List[str], List[Tuple[int, int]]]:\n",
    "    \"\"\"Tokenização simples por whitespace preservando spans (start/end).\"\"\"\n",
    "    tokens = []\n",
    "    spans = []\n",
    "    for m in re.finditer(r\"\\S+\", text):\n",
    "        tokens.append(m.group())\n",
    "        spans.append((m.start(), m.end()))\n",
    "    return tokens, spans\n",
    "\n",
    "def ensure_bio(tags: List[Any]) -> List[str]:\n",
    "    \"\"\"Normaliza uma sequência de labels para BIO.\n",
    "\n",
    "    Aceita labels como:\n",
    "    - 'O'\n",
    "    - 'PER' (sem BIO)  -> vira B-PER/I-PER dependendo da continuidade\n",
    "    - 'B-PER', 'I-PER' -> mantém (corrigindo I inválido para B quando necessário)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    prev_type = \"O\"\n",
    "    for t in tags:\n",
    "        if t is None:\n",
    "            t = \"O\"\n",
    "        t = str(t).strip()\n",
    "        if t == \"\" or t.upper() == \"O\":\n",
    "            out.append(\"O\")\n",
    "            prev_type = \"O\"\n",
    "            continue\n",
    "\n",
    "        # já vem em BIO?\n",
    "        if t.startswith(\"B-\") or t.startswith(\"I-\"):\n",
    "            pref = t[:2]  # 'B-' ou 'I-'\n",
    "            typ = t[2:]\n",
    "            # corrige I-<X> que não segue um B-/I-<X>\n",
    "            if pref == \"I-\" and not (prev_type == f\"B-{typ}\" or prev_type == f\"I-{typ}\"):\n",
    "                out.append(f\"B-{typ}\")\n",
    "                prev_type = f\"B-{typ}\"\n",
    "            else:\n",
    "                out.append(t)\n",
    "                prev_type = t\n",
    "        else:\n",
    "            # sem BIO: decide B ou I conforme continuidade\n",
    "            typ = t\n",
    "            if prev_type.endswith(f\"-{typ}\"):\n",
    "                out.append(f\"I-{typ}\")\n",
    "                prev_type = f\"I-{typ}\"\n",
    "            else:\n",
    "                out.append(f\"B-{typ}\")\n",
    "                prev_type = f\"B-{typ}\"\n",
    "    return out\n",
    "\n",
    "def extract_records(raw: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Converte o JSON em uma lista de registros (exemplos).\"\"\"\n",
    "    if isinstance(raw, list):\n",
    "        return raw\n",
    "    if isinstance(raw, dict):\n",
    "        # chaves comuns\n",
    "        for k in [\"data\", \"examples\", \"items\", \"records\", \"annotations\"]:\n",
    "            if k in raw and isinstance(raw[k], list):\n",
    "                return raw[k]\n",
    "        # dict id -> record\n",
    "        if all(isinstance(v, dict) for v in raw.values()):\n",
    "            return list(raw.values())\n",
    "    raise ValueError(\"Formato de JSON não reconhecido. Esperava lista ou dict com lista interna.\")\n",
    "\n",
    "def parse_ner_json(json_path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Lê o JSON e devolve exemplos no formato {'tokens': [...], 'ner_tags': [...]}\"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "\n",
    "    records = extract_records(raw)\n",
    "    examples = []\n",
    "\n",
    "    # tenta capturar mapeamento id->label se existir\n",
    "    global_id2label = None\n",
    "    if isinstance(raw, dict):\n",
    "        for k in [\"id2label\", \"labels\", \"tag_names\", \"ner_tags_names\"]:\n",
    "            if k in raw and isinstance(raw[k], list):\n",
    "                global_id2label = {i: str(name) for i, name in enumerate(raw[k])}\n",
    "\n",
    "    for idx, rec in enumerate(records):\n",
    "        if not isinstance(rec, dict):\n",
    "            continue\n",
    "\n",
    "        # Caso 1: já tokenizado\n",
    "        if \"tokens\" in rec and (\"ner_tags\" in rec or \"labels\" in rec or \"tags\" in rec):\n",
    "            tokens = rec[\"tokens\"]\n",
    "            tags = rec.get(\"ner_tags\", None) or rec.get(\"labels\", None) or rec.get(\"tags\", None)\n",
    "\n",
    "            if not isinstance(tokens, list) or not isinstance(tags, list):\n",
    "                continue\n",
    "\n",
    "            # converte tokens em str\n",
    "            tokens = [str(t) for t in tokens]\n",
    "\n",
    "            # tags numéricas?\n",
    "            if len(tags) > 0 and isinstance(tags[0], int):\n",
    "                id2label = None\n",
    "                # tenta achar mapping no registro\n",
    "                for k in [\"id2label\", \"labels\", \"tag_names\", \"ner_tags_names\"]:\n",
    "                    if k in rec and isinstance(rec[k], list):\n",
    "                        id2label = {i: str(name) for i, name in enumerate(rec[k])}\n",
    "                        break\n",
    "                if id2label is None:\n",
    "                    id2label = global_id2label\n",
    "                if id2label is None:\n",
    "                    raise ValueError(\n",
    "                        \"Achei tags numéricas, mas não encontrei um mapeamento id->label no JSON.\"\n",
    "                    )\n",
    "                tags = [id2label[int(t)] for t in tags]\n",
    "            else:\n",
    "                tags = [str(t) for t in tags]\n",
    "\n",
    "            # normaliza BIO\n",
    "            tags = ensure_bio(tags)\n",
    "\n",
    "            if len(tokens) != len(tags):\n",
    "                raise ValueError(\n",
    "                    f\"Registro {idx}: len(tokens)={len(tokens)} != len(tags)={len(tags)}\"\n",
    "                )\n",
    "\n",
    "            examples.append({\"tokens\": tokens, \"ner_tags\": tags})\n",
    "            continue\n",
    "\n",
    "        # Caso 2: texto + spans de entidades\n",
    "        if \"text\" in rec and (\"entities\" in rec or \"spans\" in rec or \"annotations\" in rec):\n",
    "            text = str(rec[\"text\"])\n",
    "            ents = rec.get(\"entities\", None) or rec.get(\"spans\", None) or rec.get(\"annotations\", None)\n",
    "            if not isinstance(ents, list):\n",
    "                ents = []\n",
    "\n",
    "            tokens, spans = tokenize_with_spans(text)\n",
    "            tags = [\"O\"] * len(tokens)\n",
    "\n",
    "            # ordena spans por start\n",
    "            def _start(ent: Dict[str, Any]) -> int:\n",
    "                for k in [\"start\", \"begin\", \"start_offset\", \"inicio\"]:\n",
    "                    if k in ent:\n",
    "                        return int(ent[k])\n",
    "                return 0\n",
    "\n",
    "            ents_sorted = sorted([e for e in ents if isinstance(e, dict)], key=_start)\n",
    "\n",
    "            for ent in ents_sorted:\n",
    "                start = ent.get(\"start\", ent.get(\"begin\", ent.get(\"start_offset\", ent.get(\"inicio\", None))))\n",
    "                end = ent.get(\"end\", ent.get(\"stop\", ent.get(\"end_offset\", ent.get(\"fim\", None))))\n",
    "                label = ent.get(\"label\", ent.get(\"entity\", ent.get(\"type\", ent.get(\"tipo\", None))))\n",
    "\n",
    "                if start is None or end is None or label is None:\n",
    "                    continue\n",
    "                start = int(start); end = int(end)\n",
    "                label = str(label).strip()\n",
    "\n",
    "                # remove prefixos BIO se vierem\n",
    "                base = re.sub(r\"^(B-|I-)\", \"\", label)\n",
    "\n",
    "                # tokens que intersectam o span\n",
    "                idxs = [i for i, (s, e) in enumerate(spans) if not (e <= start or s >= end)]\n",
    "                if not idxs:\n",
    "                    continue\n",
    "\n",
    "                for j, i_tok in enumerate(idxs):\n",
    "                    pref = \"B\" if j == 0 else \"I\"\n",
    "                    tags[i_tok] = f\"{pref}-{base}\"\n",
    "\n",
    "            tags = ensure_bio(tags)\n",
    "            examples.append({\"tokens\": tokens, \"ner_tags\": tags})\n",
    "            continue\n",
    "\n",
    "        # Se não reconheceu formato, ignora (ou você pode optar por raise)\n",
    "        # print(f\"Aviso: registro {idx} em formato não reconhecido. Chaves: {list(rec.keys())}\")\n",
    "\n",
    "    if len(examples) == 0:\n",
    "        raise ValueError(\n",
    "            \"Não consegui extrair nenhum exemplo de NER do JSON.\\n\"\n",
    "            \"Verifique o formato do arquivo e ajuste o parser em parse_ner_json().\"\n",
    "        )\n",
    "\n",
    "    return examples\n",
    "\n",
    "def build_label_list(examples: List[Dict[str, Any]]) -> List[str]:\n",
    "    labels = set()\n",
    "    for ex in examples:\n",
    "        for t in ex[\"ner_tags\"]:\n",
    "            labels.add(str(t))\n",
    "    if \"O\" not in labels:\n",
    "        labels.add(\"O\")\n",
    "\n",
    "    def sort_key(lab: str):\n",
    "        if lab == \"O\":\n",
    "            return (0, \"\", 0)\n",
    "        if \"-\" in lab:\n",
    "            pref, typ = lab.split(\"-\", 1)\n",
    "        else:\n",
    "            pref, typ = \"B\", lab\n",
    "        pref_order = {\"B\": 0, \"I\": 1}.get(pref, 2)\n",
    "        return (1, typ, pref_order)\n",
    "\n",
    "    label_list = sorted(labels, key=sort_key)\n",
    "    # garante 'O' primeiro\n",
    "    if label_list[0] != \"O\":\n",
    "        label_list = [\"O\"] + [l for l in label_list if l != \"O\"]\n",
    "    return label_list\n",
    "\n",
    "\n",
    "# =========================\n",
    "# AUTO-LABEL (fallback) — se o JSON vier com ner_tags = 'O' em tudo\n",
    "# =========================\n",
    "# Por que isso existe?\n",
    "# - Se o seu JSON não tem nenhuma entidade anotada, o label_list vira apenas ['O'].\n",
    "# - Isso faz num_labels=1 e o loss vira sempre 0 (treino \"degenerado\", sem aprendizado real).\n",
    "# - Este fallback cria pseudo-labels com regras (regex) para permitir um baseline funcional.\n",
    "#\n",
    "# Melhorias (v4):\n",
    "# - Unifica placeholders TELEFONE/CELULAR/FONE/TEL -> PHONE (evita duplicar labels)\n",
    "# - Heurística mais forte para endereço (ADDR), incluindo padrões comuns do DF/Brasília (SQS, SQN, SHDF, CRN, etc.)\n",
    "# - Regex de telefone mais tolerante (aceita mascaramento com X/*/#)\n",
    "# - Essas mudanças ajudam tanto o treino quanto a redução de falsos positivos na avaliação (ver seção 9)\n",
    "\n",
    "_PUNCT_STRIP = \" \\t\\n\\r.,;:!?\\\"'()[]{}<>\"\n",
    "\n",
    "# Regexes tolerantes a mascaramento (X, *, #) mantendo o formato\n",
    "CPF_RE  = re.compile(r\"^(?:[\\dXx\\*#]{3})\\.(?:[\\dXx\\*#]{3})\\.(?:[\\dXx\\*#]{3})-(?:[\\dXx\\*#]{2})$\")\n",
    "CNPJ_RE = re.compile(r\"^(?:[\\dXx\\*#]{2})\\.(?:[\\dXx\\*#]{3})\\.(?:[\\dXx\\*#]{3})/(?:[\\dXx\\*#]{4})-(?:[\\dXx\\*#]{2})$\")\n",
    "CEP_RE  = re.compile(r\"^(?:[\\dXx\\*#]{5})-(?:[\\dXx\\*#]{3})$|^(?:[\\dXx\\*#]{8})$\")\n",
    "EMAIL_RE = re.compile(r\"^[A-Za-z0-9._%+\\-]+@[A-Za-z0-9.\\-]+\\.[A-Za-z]{2,}$\")\n",
    "\n",
    "# Telefones BR comuns (com/sem DDD, com/sem +55, com/sem hífen)\n",
    "PHONE_RE = re.compile(r\"^(?:\\+?55\\s*)?(?:\\(?\\d{2}\\)?\\s*)?(?:\\d{4,5})[-\\s]?\\d{4}$\")\n",
    "# Variante tolerante a mascaramento (X/*/#)\n",
    "PHONE_MASK_RE = re.compile(r\"^(?:\\+?55\\s*)?(?:\\(?[\\dXx\\*#]{2}\\)?\\s*)?(?:[\\dXx\\*#]{4,5})[-\\s]?[\\dXx\\*#]{4}$\")\n",
    "\n",
    "# Placeholders comuns em texto mascarado\n",
    "PLACEHOLDER_RE = re.compile(\n",
    "    r\"^[\\[\\(<]?\\s*(cpf|cnpj|email|e-mail|telefone|celular|fone|tel|nome|rg|cep|endereco|endereço)\\s*[\\]\\)>]?$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Conectores comuns em nomes\n",
    "NAME_CONNECTORS = {\"de\", \"da\", \"do\", \"dos\", \"das\", \"e\"}\n",
    "\n",
    "# Heurísticas de endereço (foco DF/Brasília + genéricos)\n",
    "ADDR_STARTERS = {\n",
    "    # DF/Brasília (bem comuns em relatos)\n",
    "    \"sqs\", \"sqn\", \"scs\", \"scln\", \"sclrn\", \"sgan\", \"sgas\", \"shdf\", \"shis\",\n",
    "    \"crn\", \"cln\", \"cls\", \"cl\", \"qi\", \"q\", \"qe\", \"qna\", \"qnb\", \"qnc\", \"qnd\", \"qne\", \"qnf\",\n",
    "    # genéricos\n",
    "    \"rua\", \"r\", \"avenida\", \"av\", \"travessa\", \"alameda\", \"rodovia\", \"br\", \"km\",\n",
    "    \"quadra\", \"qd\", \"lote\", \"lt\", \"bloco\", \"bl\", \"conjunto\", \"cj\", \"setor\", \"st\",\n",
    "    \"ap\", \"apt\", \"apartamento\", \"casa\", \"loja\", \"nº\", \"no\", \"numero\", \"número\",\n",
    "    \"bairro\"\n",
    "}\n",
    "ADDR_PARTS = {\n",
    "    # componentes frequentes de endereço\n",
    "    \"bloco\", \"bl\", \"lote\", \"lt\", \"quadra\", \"qd\", \"conjunto\", \"cj\", \"setor\", \"st\",\n",
    "    \"ap\", \"apt\", \"apartamento\", \"casa\", \"loja\", \"sul\", \"norte\", \"leste\", \"oeste\",\n",
    "    \"asa\", \"l3\", \"l2\", \"l1\", \"w3\", \"w2\", \"w1\"\n",
    "}\n",
    "ROMAN_RE = re.compile(r\"^(?=[IVXLCDM]+$)[IVXLCDM]{1,4}$\", re.IGNORECASE)\n",
    "\n",
    "def _strip_punct(tok: str) -> str:\n",
    "    return str(tok).strip(_PUNCT_STRIP)\n",
    "\n",
    "def _is_upper_short(tok: str) -> bool:\n",
    "    t = _strip_punct(tok)\n",
    "    return t.isupper() and 1 <= len(t) <= 4\n",
    "\n",
    "def _looks_like_name_token(tok: str) -> bool:\n",
    "    t = _strip_punct(tok)\n",
    "    if not t:\n",
    "        return False\n",
    "    # Exclui siglas curtas (ex: CPF, DF, SQS)\n",
    "    if _is_upper_short(t):\n",
    "        return False\n",
    "    # Primeira letra maiúscula + contém letra\n",
    "    return t[0].isupper() and any(ch.isalpha() for ch in t)\n",
    "\n",
    "def _looks_like_addr_token(tok: str) -> bool:\n",
    "    t = _strip_punct(tok)\n",
    "    if not t:\n",
    "        return False\n",
    "    lt = t.lower()\n",
    "    if lt in ADDR_PARTS or lt in ADDR_STARTERS:\n",
    "        return True\n",
    "    if ROMAN_RE.match(t):\n",
    "        return True\n",
    "    if _is_upper_short(t):\n",
    "        return True\n",
    "    # contém dígito (ex: 104, 602-607, 308)\n",
    "    if any(ch.isdigit() for ch in t):\n",
    "        return True\n",
    "    # padrões tipo \"QNL23\" etc\n",
    "    if re.match(r\"^[A-Za-z]{1,6}\\d{1,4}[A-Za-z]?$\", t):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _detect_pii_type(tok: str) -> Optional[str]:\n",
    "    t_raw = str(tok).strip()\n",
    "    t = _strip_punct(t_raw)\n",
    "\n",
    "    if not t:\n",
    "        return None\n",
    "\n",
    "    # Placeholders\n",
    "    m = PLACEHOLDER_RE.match(t)\n",
    "    if m:\n",
    "        key = m.group(1).lower().replace(\"-\", \"\")\n",
    "        if key in {\"nome\"}:\n",
    "            return \"PER\"\n",
    "        if key in {\"endereco\", \"endereço\"}:\n",
    "            return \"ADDR\"\n",
    "        if key in {\"telefone\", \"celular\", \"fone\", \"tel\"}:\n",
    "            return \"PHONE\"\n",
    "        if key in {\"email\"}:\n",
    "            return \"EMAIL\"\n",
    "        if key in {\"cpf\"}:\n",
    "            return \"CPF\"\n",
    "        if key in {\"cnpj\"}:\n",
    "            return \"CNPJ\"\n",
    "        if key in {\"cep\"}:\n",
    "            return \"CEP\"\n",
    "        if key in {\"rg\"}:\n",
    "            return \"RG\"\n",
    "        return key.upper()\n",
    "\n",
    "    # Regex fortes\n",
    "    if CPF_RE.match(t):\n",
    "        return \"CPF\"\n",
    "    if CNPJ_RE.match(t):\n",
    "        return \"CNPJ\"\n",
    "    if CEP_RE.match(t):\n",
    "        return \"CEP\"\n",
    "    if EMAIL_RE.match(t):\n",
    "        return \"EMAIL\"\n",
    "\n",
    "    # Telefone: tira caracteres extras comuns e testa\n",
    "    t_phone = re.sub(r\"[()\\s]\", \"\", t)\n",
    "    if PHONE_RE.match(t_phone) or PHONE_MASK_RE.match(t_phone):\n",
    "        return \"PHONE\"\n",
    "\n",
    "    return None\n",
    "\n",
    "def auto_label_tokens(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Gera tags BIO a partir de tokens usando regras simples (prioriza precisão).\"\"\"\n",
    "    n = len(tokens)\n",
    "    tags = [\"O\"] * n\n",
    "\n",
    "    # 1) Padrões diretos por token (CPF, CNPJ, EMAIL, PHONE, CEP, placeholders)\n",
    "    for i, tok in enumerate(tokens):\n",
    "        typ = _detect_pii_type(tok)\n",
    "        if typ:\n",
    "            tags[i] = f\"B-{typ}\"\n",
    "\n",
    "    # 2) Heurística de nome (PER) por contexto\n",
    "    #    Ex.: \"meu nome é Aline Souza\" / \"nome: Aline Souza\"\n",
    "    lower = [_strip_punct(t).lower() for t in tokens]\n",
    "\n",
    "    def label_name_from(start_idx: int):\n",
    "        \"\"\"Tenta rotular uma sequência de nome a partir de start_idx.\"\"\"\n",
    "        idxs = []\n",
    "        cap_count = 0\n",
    "        j = start_idx\n",
    "        # pega até 6 tokens (para nomes com conectores)\n",
    "        while j < n and len(idxs) < 6:\n",
    "            tok_j = tokens[j]\n",
    "            lj = lower[j]\n",
    "\n",
    "            if _looks_like_name_token(tok_j):\n",
    "                idxs.append(j)\n",
    "                cap_count += 1\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "            # conectores dentro do nome (de/da/do/dos/das/e) se seguido de token \"nomeável\"\n",
    "            if lj in NAME_CONNECTORS and idxs and (j + 1) < n and _looks_like_name_token(tokens[j + 1]):\n",
    "                idxs.append(j)\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "            break\n",
    "\n",
    "        if cap_count >= 1 and idxs:\n",
    "            # não sobrescreve um token já marcado como outro PII \"forte\"\n",
    "            if tags[idxs[0]] == \"O\":\n",
    "                tags[idxs[0]] = \"B-PER\"\n",
    "            for k in idxs[1:]:\n",
    "                if tags[k] == \"O\":\n",
    "                    tags[k] = \"I-PER\"\n",
    "\n",
    "    for i in range(n):\n",
    "        # \"nome é\" / \"nome:\" / \"nome -\"\n",
    "        if lower[i] == \"nome\" and (i + 1) < n and lower[i + 1] in {\"é\", \"eh\", \":\", \"-\"}:\n",
    "            if (i + 2) < n:\n",
    "                label_name_from(i + 2)\n",
    "\n",
    "        # \"meu nome é\"\n",
    "        if lower[i] == \"meu\" and (i + 2) < n and lower[i + 1] == \"nome\" and lower[i + 2] in {\"é\", \"eh\", \":\", \"-\"}:\n",
    "            if (i + 3) < n:\n",
    "                label_name_from(i + 3)\n",
    "\n",
    "        # \"me chamo\"\n",
    "        if lower[i] == \"me\" and (i + 1) < n and lower[i + 1] == \"chamo\":\n",
    "            if (i + 2) < n:\n",
    "                label_name_from(i + 2)\n",
    "\n",
    "        # \"sou Fulano\"\n",
    "        if lower[i] == \"sou\":\n",
    "            if (i + 1) < n:\n",
    "                label_name_from(i + 1)\n",
    "\n",
    "    # 3) Heurística de endereço (ADDR)\n",
    "    #    - Detecta inícios comuns (SQS, SQN, SHDF, CRN, Rua, Av, Quadra, etc.)\n",
    "    #    - Rotula uma \"janela\" curta de tokens que parecem parte de endereço\n",
    "    def label_addr_from(start_idx: int):\n",
    "        idxs = []\n",
    "        j = start_idx\n",
    "        while j < n and len(idxs) < 12:\n",
    "            tok_j = tokens[j]\n",
    "            clean = _strip_punct(tok_j)\n",
    "            if not clean:\n",
    "                break\n",
    "            lj = clean.lower()\n",
    "\n",
    "            if j == start_idx:\n",
    "                idxs.append(j)\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "            # aceita partes típicas de endereço\n",
    "            if _looks_like_addr_token(tok_j) or lj in NAME_CONNECTORS or lj in {\"-\", \"/\", \"–\"}:\n",
    "                idxs.append(j)\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "            break\n",
    "\n",
    "        # aplica BIO sem sobrescrever PII \"forte\"\n",
    "        if idxs:\n",
    "            if tags[idxs[0]] == \"O\":\n",
    "                tags[idxs[0]] = \"B-ADDR\"\n",
    "            for k in idxs[1:]:\n",
    "                if tags[k] == \"O\":\n",
    "                    tags[k] = \"I-ADDR\"\n",
    "\n",
    "    for i in range(n):\n",
    "        # gatilho direto (token é um starter)\n",
    "        if tags[i] == \"O\" and lower[i] in ADDR_STARTERS:\n",
    "            label_addr_from(i)\n",
    "\n",
    "        # \"na/no/em <starter>\"\n",
    "        if lower[i] in {\"na\", \"no\", \"em\"} and (i + 1) < n and tags[i + 1] == \"O\" and lower[i + 1] in ADDR_STARTERS:\n",
    "            label_addr_from(i + 1)\n",
    "\n",
    "    # 4) Normaliza BIO (corrige I inválidos etc)\n",
    "    tags = ensure_bio(tags)\n",
    "    return tags\n",
    "\n",
    "def auto_label_examples(examples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Aplica auto_label_tokens em todos os exemplos.\"\"\"\n",
    "    out = []\n",
    "    for ex in examples:\n",
    "        toks = [str(t) for t in ex[\"tokens\"]]\n",
    "        out.append({\"tokens\": toks, \"ner_tags\": auto_label_tokens(toks)})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd5cca7-bebd-4668-a674-4052dd5264b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON: data\\dados_treino_ner_250.json\n",
      "CSV : data\\amostra_com_labels_1 - Página1.csv\n",
      "N exemplos: 250\n",
      "Exemplo[0] keys: dict_keys(['tokens', 'ner_tags'])\n",
      "Tokens (primeiros 20): ['oi', 'na', 'fila', 'tinha', '42', 'pessoas', 'e', 'o', 'painel', 'ficou', 'travado', 'em', '19', 'pq', 'ninguém', 'responde', 'tá', 'complicado', 'demais']\n",
      "Tags   (primeiros 20): ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Tag distribution (top 20): [('O', 5455)]\n",
      "Total tags: 5455 | Non-O: 0\n",
      "\n",
      "⚠️ ALERTA: Seu JSON não contém nenhuma entidade anotada (só 'O').\n",
      "Isso faz num_labels=1 e o treino NÃO aprende nada (loss=0 sempre).\n",
      "Vou aplicar AUTO-LABEL por regras (regex) para criar pseudo-labels e permitir treinar um baseline.\n",
      "Se você preferir corrigir o dataset manualmente, defina AUTO_LABEL_IF_ONLY_O=False e rode de novo.\n",
      "\n",
      "Após AUTO-LABEL — Tag distribution (top 20): [('O', 4460), ('I-ADDR', 254), ('B-PER', 177), ('B-ADDR', 176), ('B-EMAIL', 110), ('B-PHONE', 104), ('I-PER', 100), ('B-CPF', 74)]\n",
      "Após AUTO-LABEL — Total tags: 5455 | Non-O: 995\n",
      "\n",
      "Labels (num_labels=8):\n",
      "['O', 'B-ADDR', 'I-ADDR', 'B-CPF', 'B-EMAIL', 'B-PER', 'I-PER', 'B-PHONE']\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4) Carregar dados de treino (JSON) e preparar Dataset HF\n",
    "# =========================\n",
    "BASE_DIRS = [\n",
    "    Path(\"/data\"),           # conforme enunciado\n",
    "    Path(\"./data\"),          # alternativa comum\n",
    "    Path(\".\"),               # diretório atual\n",
    "    Path(\"/kaggle/input\"),   # Kaggle inputs\n",
    "    Path(\"/kaggle/working\"), # Kaggle working\n",
    "    Path(\"/mnt/data\"),       # sandbox/local\n",
    "]\n",
    "\n",
    "JSON_NAME = \"dados_treino_ner_250.json\"\n",
    "CSV_NAME = \"amostra_com_labels_1 - Página1.csv\"\n",
    "\n",
    "json_path = find_file_exact_or_pattern(\n",
    "    filename=JSON_NAME,\n",
    "    patterns=[\"*treino*ner*250*.json\", \"*dados*treino*ner*.json\"],\n",
    "    base_dirs=BASE_DIRS,\n",
    ")\n",
    "\n",
    "csv_path = find_file_exact_or_pattern(\n",
    "    filename=CSV_NAME,\n",
    "    patterns=[\n",
    "        \"*amostra_com_labels_1*Página1*.csv\",\n",
    "        \"*amostra_com_labels_1*Pagina1*.csv\",\n",
    "        \"*amostra*labels*Página1*.csv\",\n",
    "        \"*amostra*labels*Pagina1*.csv\",\n",
    "    ],\n",
    "    base_dirs=BASE_DIRS,\n",
    ")\n",
    "\n",
    "print(\"JSON:\", json_path)\n",
    "print(\"CSV :\", csv_path)\n",
    "\n",
    "examples = parse_ner_json(json_path)\n",
    "print(\"N exemplos:\", len(examples))\n",
    "print(\"Exemplo[0] keys:\", examples[0].keys())\n",
    "print(\"Tokens (primeiros 20):\", examples[0][\"tokens\"][:20])\n",
    "print(\"Tags   (primeiros 20):\", examples[0][\"ner_tags\"][:20])\n",
    "\n",
    "\n",
    "# Sanity check: distribuição de tags\n",
    "tag_counts = Counter(t for ex in examples for t in ex[\"ner_tags\"])\n",
    "non_o = sum(c for t, c in tag_counts.items() if t != \"O\")\n",
    "print(\"\\nTag distribution (top 20):\", tag_counts.most_common(20))\n",
    "print(\"Total tags:\", sum(tag_counts.values()), \"| Non-O:\", non_o)\n",
    "\n",
    "# Se seu JSON veio TODO 'O', o treino fica degenerado (num_labels=1 => loss=0 sempre).\n",
    "# Se isso acontecer, você tem 2 opções:\n",
    "#  (A) Corrigir o JSON para conter entidades anotadas (recomendado).\n",
    "#  (B) Usar o fallback AUTO-LABEL abaixo (baseline rápido).\n",
    "AUTO_LABEL_IF_ONLY_O = True\n",
    "\n",
    "if non_o == 0:\n",
    "    msg = (\n",
    "        \"\\n⚠️ ALERTA: Seu JSON não contém nenhuma entidade anotada (só 'O').\\n\"\n",
    "        \"Isso faz num_labels=1 e o treino NÃO aprende nada (loss=0 sempre).\\n\"\n",
    "        \"Vou aplicar AUTO-LABEL por regras (regex) para criar pseudo-labels e permitir treinar um baseline.\\n\"\n",
    "        \"Se você preferir corrigir o dataset manualmente, defina AUTO_LABEL_IF_ONLY_O=False e rode de novo.\\n\"\n",
    "    )\n",
    "    print(msg)\n",
    "    if AUTO_LABEL_IF_ONLY_O:\n",
    "        examples = auto_label_examples(examples)\n",
    "        tag_counts = Counter(t for ex in examples for t in ex[\"ner_tags\"])\n",
    "        non_o = sum(c for t, c in tag_counts.items() if t != \"O\")\n",
    "        print(\"Após AUTO-LABEL — Tag distribution (top 20):\", tag_counts.most_common(20))\n",
    "        print(\"Após AUTO-LABEL — Total tags:\", sum(tag_counts.values()), \"| Non-O:\", non_o)\n",
    "    else:\n",
    "        raise ValueError(\"JSON sem entidades (apenas 'O'). Corrija o dataset ou ative AUTO_LABEL_IF_ONLY_O.\")\n",
    "\n",
    "label_list = build_label_list(examples)\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "print(\"\\nLabels (num_labels=%d):\" % len(label_list))\n",
    "print(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dcb2775-9a4b-4b81-8c70-3fa27de4f5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 225\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 25\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d037483f0334f94a9c5d936ca09ae95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de9a9d9cf7f426aa719ef1d5947fed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 225\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 25\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Tokenização + alinhamento de labels (BIO) e split treino/val\n",
    "# =========================\n",
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "dataset = Dataset.from_list(examples).shuffle(seed=SEED)\n",
    "# Split pequeno só para monitorar loss/métricas durante treino\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=SEED)\n",
    "dataset = DatasetDict({\"train\": dataset[\"train\"], \"validation\": dataset[\"test\"]})\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "def tokenize_and_align_labels(batch):\n",
    "    tokenized = tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        # padding é feito pelo DataCollator\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i in range(len(batch[\"tokens\"])):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        word_labels = batch[\"ner_tags\"][i]\n",
    "        word_label_ids = [label2id[str(l)] for l in word_labels]\n",
    "\n",
    "        aligned = []\n",
    "        prev_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned.append(-100)\n",
    "            elif word_id != prev_word_id:\n",
    "                aligned.append(word_label_ids[word_id])\n",
    "            else:\n",
    "                # subword: ignora na loss (estratégia padrão)\n",
    "                aligned.append(-100)\n",
    "            prev_word_id = word_id\n",
    "\n",
    "        labels.append(aligned)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "print(tokenized_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2262ae60-6f19-413d-8c78-4559c3cce632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86f17d70dea4f539ec8a3d4f0970b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "BertForTokenClassification LOAD REPORT from: neuralmind/bert-base-portuguese-cased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "classifier.weight                          | MISSING    | \n",
      "classifier.bias                            | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x1e57b511370>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# 6) Modelo + Trainer\n",
    "# =========================\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ").to(DEVICE)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        seq_true = []\n",
    "        seq_pred = []\n",
    "        for p, l in zip(pred_seq, label_seq):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            seq_true.append(id2label[int(l)])\n",
    "            seq_pred.append(id2label[int(p)])\n",
    "        true_labels.append(seq_true)\n",
    "        true_preds.append(seq_pred)\n",
    "\n",
    "    # seqeval pode avisar quando não há amostras positivas. Mantemos robusto com zero_division=0 quando disponível.\n",
    "    try:\n",
    "        prec = precision_score(true_labels, true_preds, zero_division=0)\n",
    "        rec  = recall_score(true_labels, true_preds, zero_division=0)\n",
    "        f1v  = f1_score(true_labels, true_preds, zero_division=0)\n",
    "    except TypeError:\n",
    "        prec = precision_score(true_labels, true_preds)\n",
    "        rec  = recall_score(true_labels, true_preds)\n",
    "        f1v  = f1_score(true_labels, true_preds)\n",
    "\n",
    "    return {\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1v,\n",
    "        \"accuracy\": seqeval_accuracy(true_labels, true_preds),\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pii_ner_bertpt\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec76ec03-53c5-4bfb-94c4-a1116cf8ea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\GithubHD\\nlp-acesso-a-informacao\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='145' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [145/145 04:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.722163</td>\n",
       "      <td>0.515780</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.825480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.282031</td>\n",
       "      <td>0.196516</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.762590</td>\n",
       "      <td>0.952880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.119191</td>\n",
       "      <td>0.086690</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.977312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.067580</td>\n",
       "      <td>0.052428</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.989529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.044954</td>\n",
       "      <td>0.045086</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.993019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abb9b6f6a0c4617b15792cf103ed320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\GithubHD\\nlp-acesso-a-informacao\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29da23021c34bb59bf873f0237e6439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\GithubHD\\nlp-acesso-a-informacao\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532a9676ac2b4b4cb4a633c371778626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\GithubHD\\nlp-acesso-a-informacao\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d0aaf8f2a143309138e05246f5103a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\GithubHD\\nlp-acesso-a-informacao\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357f59ca612a4e3080ea7883c83a9530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.beta', 'bert.embeddings.LayerNorm.gamma', 'bert.encoder.layer.0.attention.output.LayerNorm.beta', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma', 'bert.encoder.layer.0.output.LayerNorm.beta', 'bert.encoder.layer.0.output.LayerNorm.gamma', 'bert.encoder.layer.1.attention.output.LayerNorm.beta', 'bert.encoder.layer.1.attention.output.LayerNorm.gamma', 'bert.encoder.layer.1.output.LayerNorm.beta', 'bert.encoder.layer.1.output.LayerNorm.gamma', 'bert.encoder.layer.2.attention.output.LayerNorm.beta', 'bert.encoder.layer.2.attention.output.LayerNorm.gamma', 'bert.encoder.layer.2.output.LayerNorm.beta', 'bert.encoder.layer.2.output.LayerNorm.gamma', 'bert.encoder.layer.3.attention.output.LayerNorm.beta', 'bert.encoder.layer.3.attention.output.LayerNorm.gamma', 'bert.encoder.layer.3.output.LayerNorm.beta', 'bert.encoder.layer.3.output.LayerNorm.gamma', 'bert.encoder.layer.4.attention.output.LayerNorm.beta', 'bert.encoder.layer.4.attention.output.LayerNorm.gamma', 'bert.encoder.layer.4.output.LayerNorm.beta', 'bert.encoder.layer.4.output.LayerNorm.gamma', 'bert.encoder.layer.5.attention.output.LayerNorm.beta', 'bert.encoder.layer.5.attention.output.LayerNorm.gamma', 'bert.encoder.layer.5.output.LayerNorm.beta', 'bert.encoder.layer.5.output.LayerNorm.gamma', 'bert.encoder.layer.6.attention.output.LayerNorm.beta', 'bert.encoder.layer.6.attention.output.LayerNorm.gamma', 'bert.encoder.layer.6.output.LayerNorm.beta', 'bert.encoder.layer.6.output.LayerNorm.gamma', 'bert.encoder.layer.7.attention.output.LayerNorm.beta', 'bert.encoder.layer.7.attention.output.LayerNorm.gamma', 'bert.encoder.layer.7.output.LayerNorm.beta', 'bert.encoder.layer.7.output.LayerNorm.gamma', 'bert.encoder.layer.8.attention.output.LayerNorm.beta', 'bert.encoder.layer.8.attention.output.LayerNorm.gamma', 'bert.encoder.layer.8.output.LayerNorm.beta', 'bert.encoder.layer.8.output.LayerNorm.gamma', 'bert.encoder.layer.9.attention.output.LayerNorm.beta', 'bert.encoder.layer.9.attention.output.LayerNorm.gamma', 'bert.encoder.layer.9.output.LayerNorm.beta', 'bert.encoder.layer.9.output.LayerNorm.gamma', 'bert.encoder.layer.10.attention.output.LayerNorm.beta', 'bert.encoder.layer.10.attention.output.LayerNorm.gamma', 'bert.encoder.layer.10.output.LayerNorm.beta', 'bert.encoder.layer.10.output.LayerNorm.gamma', 'bert.encoder.layer.11.attention.output.LayerNorm.beta', 'bert.encoder.layer.11.attention.output.LayerNorm.gamma', 'bert.encoder.layer.11.output.LayerNorm.beta', 'bert.encoder.layer.11.output.LayerNorm.gamma'].\n",
      "D:\\GithubHD\\nlp-acesso-a-informacao\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=145, training_loss=0.2900570425493964, metrics={'train_runtime': 248.6754, 'train_samples_per_second': 4.524, 'train_steps_per_second': 0.583, 'total_flos': 38643395404992.0, 'train_loss': 0.2900570425493964, 'epoch': 5.0})\n",
      "\n",
      "Avaliação no split de validação (do JSON):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04508648067712784, 'eval_precision': 0.9594594594594594, 'eval_recall': 0.9861111111111112, 'eval_f1': 0.9726027397260274, 'eval_accuracy': 0.9930191972076788, 'eval_runtime': 0.9197, 'eval_samples_per_second': 27.182, 'eval_steps_per_second': 4.349, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 7) Treinamento\n",
    "# =========================\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "\n",
    "print(\"\\nAvaliação no split de validação (do JSON):\")\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90034dea-fccf-4c8c-9f6d-87ffaf08fa49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d516b72cc0473483b7248d973e158e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo salvo em: D:\\GithubHD\\nlp-acesso-a-informacao\\trained_ner_model\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 8) Salvar modelo treinado (para pipeline)\n",
    "# =========================\n",
    "\n",
    "# Altere o nome do caminho se ja existir\n",
    "\n",
    "SAVE_DIR = Path(\"./trained_ner_model\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trainer.save_model(str(SAVE_DIR))\n",
    "tokenizer.save_pretrained(str(SAVE_DIR))\n",
    "\n",
    "print(\"Modelo salvo em:\", SAVE_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8f267-b6e8-4cd3-aaf3-0995ff57e32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
